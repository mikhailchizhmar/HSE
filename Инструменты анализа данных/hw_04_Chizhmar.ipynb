{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ff5c3c",
   "metadata": {},
   "source": [
    "# Перемешанные фотографии\n",
    "\n",
    "Возьмите [здесь](https://www.kaggle.com/datasets/amitsharma11jan/caltech-101/data) данные) Дальше ковыряться будем с ними :)\n",
    "\n",
    "Представим, что у вас они теперь лежат в папке, которая называется **101_ObjectCategories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe3ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c34f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '.\\\\101_ObjectCategories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a67d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ищем картинки\n",
    "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
    "def get_file_list(root_dir):\n",
    "    file_list = []\n",
    "    counter = 1\n",
    "    for root, directories, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if any(ext in filename for ext in extensions):\n",
    "                file_list.append(os.path.join(root, filename))\n",
    "                counter += 1\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fe2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(get_file_list(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fba43",
   "metadata": {},
   "source": [
    "Выберите 2000 случайных фотографий, перемешайте их названия, захешировав. Например так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc9c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(52)\n",
    "random2000 = np.random.choice(filenames, 2000)\n",
    "\n",
    "def random_hash_name(filename):\n",
    "    return '.\\\\Renamed\\\\' + hashlib.md5((filename+str(np.random.random())).encode('utf-8')).hexdigest()+'.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2b8216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\Renamed\\\\951e11facc8a0d7aa22ff63f832fccfc.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_hash_name(random2000[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9ddd1",
   "metadata": {},
   "source": [
    "Пересохраните выбранные 2000 фотографий в соседнюю папку с новыми именами. а теперь задание:\n",
    "    \n",
    "1) Используя знания об ANN или любой другой удобный вам подход, восстановите соответствие между названием исходной фотографии и переименованной\n",
    "\n",
    "2) Проверьте качество восстановления\n",
    "\n",
    "3) С каким качеством получится восстановить соответствие, если каждую фотографию после запуска процедуры немного \"испортить\", например, добавить в нее случайный \"шум\", но так чтобы картинку все еще можно было легко узнать, или запустить на ней SVD и выбрать столько главных компонент, чтобы картинка была хорошо узнаваема? \n",
    "\n",
    "4) Проверьте качество восстановления на зашумленных любым способом данных\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252a7b3d-3dc9-4038-91d1-6cdc3ce68ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_new_filenames = {}\n",
    "\n",
    "for file in random2000:\n",
    "    old_new_filenames[file] = random_hash_name(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5680a47-c1ed-42bb-aed4-b91df0528c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Renamed', exist_ok=True)\n",
    "\n",
    "for old_file, new_file in old_new_filenames.items():\n",
    "    shutil.copy2(old_file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0953758-f27f-4a13-b3b4-9993405505bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd04cb7-340e-4065-8419-f0dcfd6727da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, bins=(8, 8, 8)):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a20235c-be13-4114-807f-9047c81b7075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of recovery: 100.00%\n"
     ]
    }
   ],
   "source": [
    "old_features = []\n",
    "new_features = []\n",
    "old_paths = []\n",
    "new_paths = []\n",
    "\n",
    "for old_path, new_path in old_new_filenames.items():\n",
    "    old_feat = extract_features(old_path)\n",
    "    new_feat = extract_features(new_path)\n",
    "    if old_feat is not None and new_feat is not None:\n",
    "        old_features.append(old_feat)\n",
    "        new_features.append(new_feat)\n",
    "        old_paths.append(old_path)\n",
    "        new_paths.append(new_path)\n",
    "\n",
    "old_features = np.array(old_features)\n",
    "new_features = np.array(new_features)\n",
    "\n",
    "nn_model = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(old_features)\n",
    "distances, indices = nn_model.kneighbors(new_features)\n",
    "\n",
    "recovered_mapping = {}\n",
    "for new_idx, old_idx in enumerate(indices.flatten()):\n",
    "    recovered_mapping[new_paths[new_idx]] = old_paths[old_idx]\n",
    "\n",
    "correct_matches = sum(recovered_mapping[new] == old for old, new in old_new_filenames.items())\n",
    "accuracy = correct_matches / len(old_new_filenames)\n",
    "print(f\"Accuracy of recovery: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc066399-dfad-4983-9270-00d88b1a4344",
   "metadata": {},
   "source": [
    "Ожидаемый результат, так как у нас векторы признаков исходных и хэшированных фотографий одинаковые, и KNN легко с ними справляется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66b3fba-6aac-4d32-8bbf-e4371aabd54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image, noise_level=150):\n",
    "    noise = np.random.randint(-noise_level, noise_level, image.shape, dtype='int16')\n",
    "    noisy_image = np.clip(image + noise, 0, 255)\n",
    "    return noisy_image.astype('uint8')\n",
    "\n",
    "\n",
    "def compress_svd(image, num_components=20):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    u, s, vt = np.linalg.svd(gray_image, full_matrices=False)\n",
    "    compressed_img = np.dot(u[:, :num_components], np.dot(np.diag(s[:num_components]), vt[:num_components, :]))\n",
    "    return np.clip(compressed_img, 0, 255).astype('uint8')\n",
    "\n",
    "\n",
    "def extract_features_with_modifications(image_path, size=(32, 32), modify=None):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, size)\n",
    "    if modify == 'noise':\n",
    "        image = add_noise(image)\n",
    "    elif modify == 'svd':\n",
    "        image = compress_svd(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    return image.flatten()\n",
    "\n",
    "\n",
    "def evaluate_with_modifications(modify=None):\n",
    "    modified_old_features = []\n",
    "    modified_new_features = []\n",
    "    \n",
    "    for old_path, new_path in old_new_filenames.items():\n",
    "        old_feat = extract_features_with_modifications(old_path)\n",
    "        new_feat = extract_features_with_modifications(new_path, modify=modify)\n",
    "        if old_feat is not None and new_feat is not None:\n",
    "            modified_old_features.append(old_feat)\n",
    "            modified_new_features.append(new_feat)\n",
    "    \n",
    "    nn_model = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(modified_old_features)\n",
    "    distances, indices = nn_model.kneighbors(modified_new_features)\n",
    "    \n",
    "    recovered_mapping = {}\n",
    "    for new_idx, old_idx in enumerate(indices.flatten()):\n",
    "        recovered_mapping[new_paths[new_idx]] = old_paths[old_idx]\n",
    "    \n",
    "    correct_matches = sum(recovered_mapping[new] == old for old, new in old_new_filenames.items())\n",
    "    accuracy = correct_matches / len(old_new_filenames)\n",
    "    print(f\"Accuracy with {modify or 'no modifications'}: {accuracy:.2%}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e115962f-c2a1-46fa-a33e-1681ec5f0923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with noise: 99.83%\n",
      "Accuracy with svd: 97.14%\n"
     ]
    }
   ],
   "source": [
    "accuracy_with_noise = evaluate_with_modifications(modify='noise')\n",
    "accuracy_with_svd = evaluate_with_modifications(modify='svd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0c69d-53c5-4f25-b506-6d0c541fb366",
   "metadata": {},
   "source": [
    "Видим, что при стремительнм увеличении шума accuracy уменьшается не так быстро. С сингулярным разложением другая история - accuracy сильно уменьшается при небольшом уменьшении количества главных компонент.  \n",
    "В нашем случае, при использовании SVD с 20 главными компонентами мы всё равно отлично восстанавливаем схожесть между исходными и хэшированными фотографиями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c07e1",
   "metadata": {},
   "source": [
    "# Реализация Любой версии LogLog :)\n",
    "\n",
    "Некоторое время назад мы обсуждали с вами реализацию алгоритмов семейства LogLog, где для расчета кардинальности множества подсчитывалось количество ведущих нулей в hash'е от элемента, а дальше на основании максимального кол-ва нулей, которое мы в множестве увидели, подсчитывалось количество элементов.\n",
    "\n",
    "Далее товарищ Флажоле придумал это делать несколько раз и усреднять.\n",
    "\n",
    "Как ведущие нули посчитать, можно [нагуглить](https://stackoverflow.com/questions/71888646/counting-the-number-of-leading-zero-bits-in-a-sha256-encrpytion)\n",
    "\n",
    "Задание: реализуйте любой из алгоритмов семейства LogLog (пусть даже самого простого Флажоле-Мартена) и оцените его уровень ошибки на случайно сгенерированном множестве из случайных строк с повторениями "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38317229-7df1-4eb9-8078-64e6cb05c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b996c765-ff20-4580-bc04-e88edb7013d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function(item):\n",
    "    return int(hashlib.md5(item.encode('utf-8')).hexdigest(), 16)\n",
    "\n",
    "\n",
    "def count_leading_zeros(x, max_bits):\n",
    "    binary = bin(x)[2:].zfill(max_bits)\n",
    "    return binary.find('1')\n",
    "\n",
    "\n",
    "def flajolet_martin(stream, num_hash_functions=10):\n",
    "    max_bits = 128\n",
    "    max_zeros = np.zeros(num_hash_functions, dtype=int)\n",
    "\n",
    "    for item in stream:\n",
    "        for i in range(num_hash_functions):\n",
    "            combined_item = f\"{item}_{i}\"\n",
    "            hashed_value = hash_function(combined_item)\n",
    "            leading_zeros = count_leading_zeros(hashed_value, max_bits)\n",
    "            max_zeros[i] = max(max_zeros[i], leading_zeros)\n",
    "\n",
    "    estimates = 2 ** max_zeros\n",
    "    return np.mean(estimates)\n",
    "\n",
    "\n",
    "def generate_random_strings(size, string_length=10):\n",
    "    return [''.join(np.random.choice(list(string.ascii_letters), size=string_length)) for _ in range(size)]\n",
    "\n",
    "\n",
    "def evaluate_error(true_cardinality, estimated_cardinality):\n",
    "    return abs(estimated_cardinality - true_cardinality) / true_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcaa6dc4-2449-4244-a22b-a265ee1a028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Посчитанная кардинальность: 576.0\n",
      "Истинная кардинальность:    500\n",
      "Относительная ошибка:       0.152\n"
     ]
    }
   ],
   "source": [
    "stream_size = 1000\n",
    "unique_elements = 500\n",
    "num_hash_functions = 10\n",
    "\n",
    "np.random.seed(9)\n",
    "unique_stream = generate_random_strings(unique_elements)\n",
    "random_stream = unique_stream + list(np.random.choice(unique_stream, size=stream_size - unique_elements))\n",
    "np.random.shuffle(random_stream)\n",
    "\n",
    "true_cardinality = len(set(random_stream))\n",
    "estimated_cardinality = flajolet_martin(random_stream, num_hash_functions=num_hash_functions)\n",
    "error = evaluate_error(true_cardinality, estimated_cardinality)\n",
    "\n",
    "print(f\"Посчитанная кардинальность: {estimated_cardinality}\")\n",
    "print(f\"Истинная кардинальность:    {true_cardinality}\")\n",
    "print(f\"Относительная ошибка:       {error:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
